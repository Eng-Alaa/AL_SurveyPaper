{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cdf5d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dataclasses import replace\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.special import entr\n",
    "from sklearn.metrics import accuracy_score\n",
    "import operator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#from modAL.models import ActiveLearner\n",
    "# Set our RNG seed for reproducibility.\n",
    "RANDOM_STATE_SEED = 123\n",
    "np.random.seed(RANDOM_STATE_SEED)\n",
    "\n",
    "iris = load_iris()\n",
    "X_raw = iris['data']\n",
    "y_raw = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "41e7aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset to include only the first two classes\n",
    "mask = (y_raw == 1) | (y_raw == 2)\n",
    "X = X_raw[mask]\n",
    "y = y_raw[mask]\n",
    "X_raw=X\n",
    "y_raw=y\n",
    "pca = PCA(n_components=2)\n",
    "transformed_iris = pca.fit_transform(X=X_raw)\n",
    "\n",
    "# Isolate the data we'll need for plotting.\n",
    "x_component, y_component = transformed_iris[:, 0], transformed_iris[:, 1]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.6, random_state=42)\n",
    "n_labeled_examples = X_train.shape[0]\n",
    "#print(X_train)\n",
    "#print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1b596f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8 69 50]\n",
      "(3, 4)\n",
      "(3,)\n",
      "(37, 4)\n",
      "(37,)\n"
     ]
    }
   ],
   "source": [
    "# pick three points randomly and make their labels available (initial training data)\n",
    "initial_indices=np.random.permutation(n_labeled_examples-1)[0:3]\n",
    "print(training_indices)\n",
    "\n",
    "# original training and pool\n",
    "X_Labelled = X_train[initial_indices]\n",
    "y_Labelled = y_train[initial_indices]\n",
    "\n",
    "# Isolate the non-training examples we'll be querying.\n",
    "X_pool = np.delete(X_train, initial_indices, axis=0)\n",
    "y_pool = np.delete(y_train, initial_indices, axis=0)\n",
    "\n",
    "print(X_Labelled.shape)\n",
    "print(y_Labelled.shape)\n",
    "print(X_pool.shape)\n",
    "print(y_pool.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2b12af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pool_entropy=X_pool\n",
    "X_pool_dist=X_pool\n",
    "X_pool_r=X_pool\n",
    "\n",
    "y_pool_entropy=y_pool\n",
    "y_pool_dist=y_pool\n",
    "y_pool_r=y_pool\n",
    "\n",
    "X_Labelled_entropy=X_Labelled\n",
    "X_Labelled_dist=X_Labelled\n",
    "X_Labelled_r=X_Labelled\n",
    "\n",
    "y_Labelled_entropy=y_Labelled\n",
    "y_Labelled_dist=y_Labelled\n",
    "y_Labelled_r=y_Labelled\n",
    "\n",
    "initial_indices_entropy=initial_indices\n",
    "initial_indices_dist=initial_indices\n",
    "initial_indices_r=initial_indices\n",
    "\n",
    "clf_entropy = RandomForestClassifier(max_depth = 4, min_samples_split=2, n_estimators = 200, random_state = 1) \n",
    "clf_dist = RandomForestClassifier(max_depth = 4, min_samples_split=2, n_estimators = 200, random_state = 1) \n",
    "clf_r = RandomForestClassifier(max_depth = 4, min_samples_split=2, n_estimators = 200, random_state = 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1975089c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most uncertain point (entropy-based) is No. 19\n",
      "The distance-based point is No. 10\n",
      "The point randomly is No. 21\n",
      "The most uncertain point (entropy-based) is No. 31\n",
      "The distance-based point is No. 5\n",
      "The point randomly is No. 30\n",
      "The most uncertain point (entropy-based) is No. 17\n",
      "The distance-based point is No. 3\n",
      "The point randomly is No. 27\n",
      "The most uncertain point (entropy-based) is No. 0\n",
      "The distance-based point is No. 4\n",
      "The point randomly is No. 33\n",
      "The most uncertain point (entropy-based) is No. 22\n",
      "The distance-based point is No. 0\n",
      "The point randomly is No. 12\n",
      "The most uncertain point (entropy-based) is No. 18\n",
      "The distance-based point is No. 11\n",
      "The point randomly is No. 8\n",
      "The most uncertain point (entropy-based) is No. 20\n",
      "The distance-based point is No. 14\n",
      "The point randomly is No. 3\n",
      "The most uncertain point (entropy-based) is No. 16\n",
      "The distance-based point is No. 2\n",
      "The point randomly is No. 10\n",
      "The most uncertain point (entropy-based) is No. 24\n",
      "The distance-based point is No. 26\n",
      "The point randomly is No. 5\n",
      "The most uncertain point (entropy-based) is No. 24\n",
      "The distance-based point is No. 6\n",
      "The point randomly is No. 0\n",
      "[0.7833333333333333, 0.8333333333333334, 0.8666666666666667, 0.9, 0.7833333333333333, 0.85, 0.9166666666666666, 0.9333333333333333, 0.9666666666666667, 0.9]\n",
      "[0.7833333333333333, 0.8166666666666667, 0.8166666666666667, 0.8, 0.8, 0.7333333333333333, 0.7, 0.7333333333333333, 0.7166666666666667, 0.7166666666666667]\n",
      "[0.7833333333333333, 0.85, 0.85, 0.8333333333333334, 0.9, 0.9, 0.9, 0.9, 0.9, 0.8833333333333333]\n"
     ]
    }
   ],
   "source": [
    "N_QUERIES = 10\n",
    "Accuracy_entorpy=[]\n",
    "Accuracy_dist=[]\n",
    "Accuracy_random=[]\n",
    "\n",
    "# Allow our model to query our unlabeled dataset for the most\n",
    "# informative points according to our query strategy (uncertainty sampling).\n",
    "\n",
    "for index in range(N_QUERIES):\n",
    "    clf_entropy.fit(X_Labelled_entropy, y_Labelled_entropy) \n",
    "    predictions_entropy= clf_entropy.predict(X_test) \n",
    "    Accuracy_entorpy.append(accuracy_score(predictions_entropy, y_test.reshape(-1,1)))\n",
    "    predicted_probs_entropy = clf_entropy.predict_proba(X_pool_entropy) \n",
    "    ## Uncertainty sampling (entropy-based)\n",
    "    entropy_array = entr(predicted_probs_entropy).sum(axis=1)\n",
    "    query_index_entropy, value_entropy = max(enumerate(entropy_array), key=operator.itemgetter(1))\n",
    "    print('The most uncertain point (entropy-based) is No.', query_index_entropy)\n",
    "\n",
    "    # add the new labeled point to the training data\n",
    "    initial_indices_entropy =np.append(initial_indices_entropy,query_index_entropy)\n",
    "    X, y = X_pool_entropy[query_index_entropy].reshape(1, -1), y_pool_entropy[query_index_entropy].reshape(1, )\n",
    "    \n",
    "    X_Labelled_entropy=np.concatenate((X_Labelled_entropy,X),axis=0)\n",
    "    y_Labelled_entropy= np.append(y_Labelled_entropy,y)\n",
    "    # Remove the queried instance from the unlabeled pool.\n",
    "    X_pool_entropy, y_pool_entropy = np.delete(X_pool_entropy, query_index_entropy, axis=0), np.delete(y_pool_entropy, query_index_entropy)\n",
    "\n",
    "    # Distance-based model\n",
    "    clf_dist.fit(X_Labelled_dist, y_Labelled_dist) \n",
    "    predictions_dist= clf_dist.predict(X_test) \n",
    "    Accuracy_dist.append(accuracy_score(predictions_dist, y_test.reshape(-1,1)))\n",
    "    distances = []\n",
    "    for x_pool in X_pool_dist:\n",
    "        distances.append(np.mean([np.linalg.norm(x_pool - x_labeled) for x_labeled in X_Labelled_dist]))\n",
    "    query_index_dist = np.argmax(distances)\n",
    "    \n",
    "    print('The distance-based point is No.', query_index_dist)\n",
    "\n",
    "    # add the new labeled point to the training data\n",
    "    initial_indices_dist =np.append(initial_indices_dist,query_index_dist)\n",
    "    X, y = X_pool_dist[query_index_dist].reshape(1, -1), y_pool_dist[query_index_dist].reshape(1, )\n",
    "    \n",
    "    X_Labelled_dist=np.concatenate((X_Labelled_dist,X),axis=0)\n",
    "    y_Labelled_dist= np.append(y_Labelled_dist,y)\n",
    "    # Remove the queried instance from the unlabeled pool.\n",
    "    X_pool_dist, y_pool_dist = np.delete(X_pool_dist, query_index_dist, axis=0), np.delete(y_pool_dist, query_index_dist)\n",
    "    \n",
    "    # Random method\n",
    "    clf_r.fit(X_Labelled_r, y_Labelled_r) \n",
    "    predictions_r= clf_r.predict(X_test) \n",
    "    Accuracy_random.append(accuracy_score(predictions_r, y_test.reshape(-1,1)))\n",
    "    random_index = np.random.randint(0, len(X_pool_r))\n",
    "    print('The point randomly is No.', random_index)\n",
    "\n",
    "    # add the new labeled point to the training data\n",
    "    initial_indices_r =np.append(initial_indices_r,random_index)\n",
    "    X, y = X_pool_r[random_index].reshape(1, -1), y_pool_r[random_index].reshape(1, )\n",
    "    \n",
    "    X_Labelled_r=np.concatenate((X_Labelled_r,X),axis=0)\n",
    "    y_Labelled_r= np.append(y_Labelled_r,y)\n",
    "    # Remove the queried instance from the unlabeled pool.\n",
    "    X_pool_r, y_pool_r = np.delete(X_pool_r, random_index, axis=0), np.delete(y_pool_r, random_index)\n",
    "\n",
    "\n",
    "print(Accuracy_entorpy)\n",
    "print(Accuracy_dist)\n",
    "print(Accuracy_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d958e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a pandas DataFrame with the accuracy results\n",
    "data = {\n",
    "    'Entropy-based': Accuracy_entorpy,\n",
    "    'Distance-based': Accuracy_dist,\n",
    "    'Random': Accuracy_random\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('results_Accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d403b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598c301d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
